Cross-Dataset Evaluation of Imbalance Handling Strategies for Software Defect Prediction Using Machine Learning

Dhruv Sharma
Independent Researcher
Email: dhruvsharmaaugust2003@gmail.com
GitHub: https://github.com/dhruv0111

Abstract

Software defect prediction plays a critical role in enabling risk-aware software testing and improving overall software quality. However, real-world defect datasets are highly imbalanced, with defective modules typically representing a small minority of the total data. This imbalance significantly affects the performance of machine learning models, particularly in detecting defect-prone modules. In this study, we investigate the impact of imbalance handling strategies on defect prediction performance across multiple publicly available NASA datasets (CM1, KC1, PC1, and JM1). Three strategies are compared: no imbalance handling, class-weighted learning, and Synthetic Minority Over-sampling Technique (SMOTE). Using Logistic Regression as the baseline classifier and employing stratified 5-fold cross-validation, we evaluate performance using Area Under the ROC Curve (AUC) and recall for defective modules. Experimental results demonstrate that class-weight balancing consistently achieves the highest recall (up to 0.72) across datasets while maintaining competitive AUC performance. Statistical testing confirms significant differences between imbalance strategies. The findings highlight the importance of imbalance-aware learning in practical defect prediction systems and provide empirical guidance for integrating machine learning into intelligent, risk-based software testing frameworks.

1. Introduction

Software quality assurance remains one of the most critical phases of the software development lifecycle. As modern software systems grow in complexity, size, and integration scope, identifying defect-prone components becomes increasingly challenging. Large-scale systems may consist of hundreds or thousands of modules, making exhaustive testing costly and time-consuming. Consequently, organizations seek intelligent mechanisms to prioritize testing efforts toward high-risk components.

Software defect prediction (SDP) has emerged as a promising research area that leverages historical software metrics to predict defect-prone modules. By applying machine learning techniques to static code metrics such as lines of code, cyclomatic complexity, and Halstead measures, researchers aim to build predictive models that can assist testers in focusing on high-risk areas.

Despite significant progress, one major challenge persists in defect prediction research: class imbalance. In most real-world datasets, defective modules constitute a small fraction of total modules. Traditional machine learning algorithms tend to favor the majority class, resulting in high overall accuracy but poor detection of defective modules. From a practical software testing perspective, missing defective modules (false negatives) is more harmful than incorrectly flagging non-defective ones.

This study systematically evaluates the effect of imbalance handling strategies across multiple datasets. Unlike single-dataset studies, we conduct cross-dataset experiments to assess generalization and robustness. The primary contributions of this work are:

A comprehensive cross-dataset evaluation using four NASA datasets.

A comparative study of imbalance handling techniques (none, class-weight, SMOTE).

Statistical validation of observed differences.

Practical implications for risk-based test automation.

2. Related Work

Software defect prediction has been widely explored in empirical software engineering. Early studies by Menzies et al. demonstrated the feasibility of using static code metrics for defect prediction. Various machine learning models including Logistic Regression, Decision Trees, Support Vector Machines, Random Forests, and Neural Networks have been applied.

A recurring issue in defect prediction datasets is severe class imbalance. Researchers have proposed techniques such as cost-sensitive learning, oversampling, undersampling, and hybrid methods to mitigate this problem. SMOTE has been widely adopted to synthetically generate minority class samples. However, empirical comparisons across multiple datasets remain limited.

This study contributes by providing a structured, cross-dataset comparison of imbalance handling strategies under consistent evaluation settings.

3. Datasets

Four publicly available datasets from the NASA Metrics Data Program (MDP) were used:

CM1

KC1

PC1

JM1

Each dataset contains software modules described using static code metrics including:

Lines of Code (LOC)

Cyclomatic Complexity

Halstead metrics

Branch count

Code/comment ratio metrics

The datasets exhibit significant class imbalance, with defective modules typically representing 10–20% of instances.

4. Methodology
4.1 Data Preprocessing

For each dataset:

Non-predictive identifiers were removed.

Target variable (“defects”) was extracted.

Features were standardized using StandardScaler.

Stratified splitting ensured class distribution preservation.

4.2 Model Selection

Logistic Regression was selected as the baseline classifier due to:

Interpretability

Stability

Widespread use in defect prediction literature

4.3 Imbalance Handling Strategies

Three strategies were evaluated:

No imbalance handling

Class-weight balancing

SMOTE oversampling

4.4 Evaluation Protocol

Stratified 5-fold cross-validation

Metrics:

AUC

Recall (defect class)

Paired t-test for statistical significance

5. Experimental Results
5.1 Imbalance Strategy Comparison
Dataset	Strategy	Mean AUC	Mean Recall
CM1	None	0.7967	0.0622
CM1	Class-weight	0.8023	0.7289
CM1	SMOTE	0.7885	0.6267
KC1	None	0.8011	0.2053
KC1	Class-weight	0.8023	0.6994
KC1	SMOTE	0.7957	0.6903
PC1	None	0.8337	0.1317
PC1	Class-weight	0.8383	0.7175
PC1	SMOTE	0.8226	0.6900
JM1	None	0.7228	0.0942
JM1	Class-weight	0.7250	0.5972
JM1	SMOTE	0.7233	0.5968

Across all datasets, class-weight balancing consistently improved defect recall while maintaining competitive AUC scores.

5.2 Statistical Analysis

A paired t-test comparing class-weight and random forest approaches yielded a p-value < 0.05, confirming statistically significant differences in performance.

6. Discussion

The results clearly demonstrate that imbalance handling is essential for practical defect prediction. Without balancing, recall for defective modules drops drastically (as low as 0.06). Class-weight balancing significantly improves defect detection performance while maintaining overall predictive stability.

Interestingly, SMOTE did not consistently outperform class-weight balancing. This suggests that algorithm-level balancing may be more stable than synthetic oversampling for defect prediction datasets.

From a software testing perspective, improving recall is more valuable than marginal gains in AUC. Missing defect-prone modules can lead to increased maintenance cost and reduced reliability.

7. Conclusion

This study conducted a cross-dataset evaluation of imbalance handling strategies for software defect prediction. Results across four NASA datasets demonstrate that class-weight balancing consistently achieves the best trade-off between recall and AUC.

The findings emphasize the necessity of imbalance-aware learning in defect prediction systems and provide actionable insights for integrating machine learning into intelligent testing frameworks.

Future work may explore:

Cross-project training and testing

Cost-sensitive learning models

Explainable AI techniques

Deep learning architectures for defect prediction

References

[1] T. Menzies et al., “Data Mining Static Code Attributes to Learn Defect Predictors,” IEEE TSE.
[2] N. Fenton and M. Neil, “A Critique of Software Defect Prediction Models,” IEEE TSE.
[3] PROMISE Repository, NASA MDP datasets.
[4] N. Chawla et al., “SMOTE: Synthetic Minority Over-sampling Technique,” JAIR.